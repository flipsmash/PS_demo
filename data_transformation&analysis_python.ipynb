{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123dfbc2-709f-450d-8d2a-5b04e6e3f2ab",
   "metadata": {},
   "source": [
    "# Preprocessing, wrangling, file creation, and analysis in Python (everything except IRT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bc6e5c-dd64-47a3-8478-1b7318ed4930",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c5ecee-8e97-4402-9d24-17701e800f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu March 7 19:09:06 2024\n",
    "@author: brian_local\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.stats import mannwhitneyu\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf22e4-05c4-486f-be5f-78c3494d5b35",
   "metadata": {},
   "source": [
    "### Import data that was cleaned in Excel, one frame for main data, one for question-to-tag correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3662d01-59ba-4ad1-8f6b-0edc39e9cd0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_main = pd.read_excel('original_data/data.xlsx', sheet_name = 0)\n",
    "df_tags = pd.read_excel('original_data/data.xlsx', sheet_name = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de89e3-9e35-4e75-b99a-d7f0a2b97e6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create calculated fields\n",
    "- Note assumption in creating passing score Boolean variable that pass percentage rounds to the nearest whole percent, i.e. that 91.6% (120/131) passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4e93bac-9ac9-4917-ac37-d5d5d66c2faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_main['score'] = df_main.loc[:,'q01-score':'q14-score'].sum(axis=1)\n",
    "df_main['score_pct'] = (df_main['score']/df_main['max_score']).round(2)\n",
    "df_main['total_q_time'] = df_main.loc[:,'q01-time':'q14-time'].sum(axis=1)\n",
    "df_main['score_pass_bool'] = np.where(df_main['score_pct']>=.92, 1, 0)\n",
    "df_main['copy_paste_bool'] = np.where(df_main['copy_paste_ct']>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be312d10-bef3-4f4b-a358-cb2e901b814d",
   "metadata": {},
   "source": [
    "### Identify duration columns (those that include 'time') and convert to hours for easier reading\n",
    "- Also created a starttime variable in the hopes that the time part of 'Attempt starttime' is legit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aab57796-4948-4124-b747-a149f92ba7cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_columns = df_main.filter(like='time').columns\n",
    "df_main[time_columns] = df_main[time_columns] / 3600\n",
    "\n",
    "df_main['attempt_time_of_day'] = df_main['attempt_start_dt'].dt.time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad564320-24ac-469e-a4a7-d52a905eee3d",
   "metadata": {},
   "source": [
    "### Create three data frames and corresponding csv files:  one with all clean data, one with test-level only, one with question-level only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd23b0f-b507-4705-8ee1-bde5b1fccfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'^q\\d+')\n",
    "test_columns = [col for col in df_main.columns if not pattern.match(col)]\n",
    "quest_columns  = [col for col in df_main.columns if pattern.match(col)]\n",
    "\n",
    "df_main.to_csv('clean_main_data.csv')\n",
    "df_test = df_main[test_columns]\n",
    "df_test.to_csv('clean_test_only_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5442e0-3473-4089-92d1-2c9bf5329a35",
   "metadata": {},
   "source": [
    "### Removing Question 1 from further analysis\n",
    "- Worth 1/10th of all other questions\n",
    "- Stands apart as only questions involving DDL/DML instead of DQL SQL\n",
    "- Appears to involve just running a script or two\n",
    "- Can't impact pass fail (as long as percents are rounded)\n",
    "- Admittedly, especially given missing values for Q1, simplifies analysis significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc7ca30-2362-4238-950b-e25d4c5d11e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_quest = df_main[quest_columns].iloc[:,1:].copy()\n",
    "df_quest['id'] = df_main['cand_id']\n",
    "df_quest.to_csv('clean_quest_only_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b839e-13c1-4abc-90ba-45cd3bbbb9df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question-level data transformation\n",
    "- Separating questions level data by category (score, duration, lines of code, compile counts)\n",
    "- Melting each to long form\n",
    "- Reassembling all categories\n",
    "- Joining (\"merging\" in pandas) to M:N question \"tags\" for tag-level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a29c47f-bf68-40e8-b27c-1258868babd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_columns = [col for col in df_quest.columns if 'score' in col]\n",
    "time_columns = [col for col in df_quest.columns if 'time' in col]\n",
    "loc_columns = [col for col in df_quest.columns if 'loc' in col]\n",
    "compile_columns = [col for col in df_quest.columns if 'compile' in col]\n",
    "\n",
    "df_scores = df_quest[score_columns].copy()\n",
    "df_times = df_quest[time_columns].copy()\n",
    "df_locs = df_quest[loc_columns].copy()\n",
    "df_compiles = df_quest[compile_columns].copy()\n",
    "\n",
    "df_scores['id'] = df_quest['id']\n",
    "df_times['id'] = df_quest['id']\n",
    "df_locs['id'] = df_quest['id']\n",
    "df_compiles['id'] = df_quest['id']\n",
    "\n",
    "df_scores_melted = pd.melt(df_scores, id_vars=['id'], var_name='question', value_name='score')\n",
    "df_times_melted = pd.melt(df_times, id_vars=['id'], var_name='question', value_name='time')\n",
    "df_locs_melted = pd.melt(df_locs, id_vars=['id'], var_name='question', value_name='loc')\n",
    "df_compiles_melted = pd.melt(df_compiles, id_vars=['id'], var_name='question', value_name='compile&test_ct')\n",
    "\n",
    "df_scores_melted['question'] = df_scores_melted['question'].str.replace('-score', '')\n",
    "df_times_melted['question'] = df_times_melted['question'].str.replace('-time', '')\n",
    "df_locs_melted['question'] = df_locs_melted['question'].str.replace('-loc', '')\n",
    "df_compiles_melted['question'] = df_compiles_melted['question'].str.replace('-compile&test_ct', '')\n",
    "\n",
    "\n",
    "df_long1 = pd.merge(df_scores_melted, df_times_melted, on=['id', 'question'])\n",
    "df_long2 = pd.merge(df_locs_melted, df_compiles_melted, on=['id', 'question'])\n",
    "df_long = pd.merge(df_long1, df_long2, on=['id', 'question'])\n",
    "df_long_w_tags = pd.merge(df_long, df_tags, left_on = \"question\", right_on = \"Qnum\", how = \"inner\")\n",
    "df_long.to_csv('clean_quest_long_data.csv')\n",
    "df_long_w_tags.to_csv('clean_quest_w_tags_long_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f563832-3a20-4110-94f9-8d68f4bcc2f9",
   "metadata": {},
   "source": [
    "__--end data transformation work--__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7da9b-7f2d-4e78-8b4e-83f980afbeb8",
   "metadata": {},
   "source": [
    "## Analysis of copy/paste and out-of-window time behavior's impact on scores\n",
    "### Treating Copy/Paste as Yes/No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "278edb91-6dfa-4b63-bbf0-89fb293b1a10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of copy/pasters 122.76\n",
      "mean of non-copy/pasters 104.1\n"
     ]
    }
   ],
   "source": [
    "group1 = df_main[df_main['copy_paste_bool'] == 1]['score']\n",
    "group2 = df_main[df_main['copy_paste_bool'] == 0]['score']\n",
    "mean_group1 = group1.mean().round(2)\n",
    "mean_group2 = group2.mean().round(2)\n",
    "print(f'Mean of copy/pasters {mean_group1}\\nmean of non-copy/pasters {mean_group2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350cd0b3-85f1-40ce-a535-1c6fc4fb0ae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Is this difference statistically significant?\n",
    "- Normally would do a simple t-test\n",
    "- Can't here, because the assumption of normality is completely violated (see Tableau viz of scores histogram)\n",
    "- Log transformation -- converting score to ln(score) -- won't fix in this case because modes are at extremes\n",
    "- Conduct nonparametric Mann-Whitney U test instead\n",
    "- Hypothesis is that copy/pasters > non-copy/pasters at alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84230ddb-56d4-434a-b62f-b3d4ebea0da0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mann-Whitney U Test Statistic: 68893.5, P-value: 2.244793123162359e-15\n"
     ]
    }
   ],
   "source": [
    "stat, p_value = mannwhitneyu(group1, group2, alternative='greater')\n",
    "print(f'Mann-Whitney U Test Statistic: {stat}, P-value: {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c01ca0-8443-4672-8c82-a655ec584f84",
   "metadata": {},
   "source": [
    "H<sub>0</sub> that µ<sub>group1</sub> = µ<sub>group2</sub> is rejected at alpha = 0.05\n",
    "There is statistical evidence that the copy-pasters score higher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ae231-e615-4138-9177-1a6aa1a972c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models that treat copy/paste and out of window counts as continuous\n",
    "- Slightly dubious given that they are counts, esp. given copy/paste ct range\n",
    "- Some stats traditions would be fine with it, others not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea02dabc-705a-4777-92f1-2b50cc8c84bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  score   R-squared:                       0.025\n",
      "Model:                            OLS   Adj. R-squared:                  0.023\n",
      "Method:                 Least Squares   F-statistic:                     16.25\n",
      "Date:                Wed, 13 Mar 2024   Prob (F-statistic):           6.20e-05\n",
      "Time:                        08:33:44   Log-Likelihood:                -3242.1\n",
      "No. Observations:                 648   AIC:                             6488.\n",
      "Df Residuals:                     646   BIC:                             6497.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "const           108.9188      1.606     67.837      0.000     105.766     112.072\n",
      "copy_paste_ct     0.3514      0.087      4.032      0.000       0.180       0.523\n",
      "==============================================================================\n",
      "Omnibus:                      270.353   Durbin-Watson:                   1.755\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              803.847\n",
      "Skew:                          -2.135   Prob(JB):                    2.80e-175\n",
      "Kurtosis:                       6.396   Cond. No.                         20.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "x = sm.add_constant(df_main['copy_paste_ct'])\n",
    "y = df_main['score']\n",
    "\n",
    "model = sm.OLS(y, x).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c6ba7-23bc-44de-befc-e09ce44d6aba",
   "metadata": {},
   "source": [
    " - The model and each of its coefficients are significant (p < .001)\n",
    " - Each instance of copy/paste increases the expected score by over one third of one point. \n",
    " - There is significant evidence of non-normally distributed error terms, however, which threatens an assumption of the test, and warrants further investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5034e-eb2c-4b30-8f5c-62bac340338e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Adding a second independent variable for window exit count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9edfc8aa-fe3d-409c-a494-f7cc6c271f06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  score   R-squared:                       0.045\n",
      "Model:                            OLS   Adj. R-squared:                  0.042\n",
      "Method:                 Least Squares   F-statistic:                     15.29\n",
      "Date:                Wed, 13 Mar 2024   Prob (F-statistic):           3.24e-07\n",
      "Time:                        08:33:44   Log-Likelihood:                -3235.1\n",
      "No. Observations:                 648   AIC:                             6476.\n",
      "Df Residuals:                     645   BIC:                             6490.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const            105.3279      1.857     56.726      0.000     101.682     108.974\n",
      "copy_paste_ct      0.3032      0.087      3.475      0.001       0.132       0.475\n",
      "window_exit_ct     0.0555      0.015      3.743      0.000       0.026       0.085\n",
      "==============================================================================\n",
      "Omnibus:                      253.134   Durbin-Watson:                   1.762\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              696.310\n",
      "Skew:                          -2.030   Prob(JB):                    6.28e-152\n",
      "Kurtosis:                       6.051   Cond. No.                         159.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "x2 = df_main[['copy_paste_ct', 'window_exit_ct']]\n",
    "\n",
    "# Again add a constant term for the intercept\n",
    "x2 = sm.add_constant(x2)\n",
    "\n",
    "model = sm.OLS(y, x2).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156247b-5fb3-4fb3-9cd1-279c536f4fc7",
   "metadata": {},
   "source": [
    "- Very similar to univariate copy/paste OLS above, with effect divided between copy/paste and window exit counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8788b1-5097-49fc-b638-57db30b47a3d",
   "metadata": {},
   "source": [
    "## Bottom Lines\n",
    "- There is significant evidence that copy/pasting is associated with improved scores\n",
    "- Plagiarism is clearly a concern given all the (invariant, negative) plagiarism columns in raw data\n",
    "- **IF** copy/paste plagiarism is a prospective issue, further investigation is warranted\n",
    "- I routinely see my homework problem set questions asked and (less frequently) answered on stack overflow\n",
    "- Not to mention, e.g., Chegg, course hero, braindump sites, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55794953-b653-4341-8b18-aa36bfdcac9a",
   "metadata": {},
   "source": [
    "# Preliminary, Basic Item-Level Stats\n",
    "Question Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "506ae61c-4bdb-4c94-a2a3-eb0189e2639c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              mean  rank\n",
      "q02-score    0.924  14.0\n",
      "q03-score    0.920  12.0\n",
      "q04-score    0.921  13.0\n",
      "q05-score    0.909  11.0\n",
      "q06-score    0.861   8.0\n",
      "q07-score    0.847   3.0\n",
      "q08-score    0.852   4.5\n",
      "q09-score    0.887  10.0\n",
      "q10-score    0.886   9.0\n",
      "q11-score    0.852   4.5\n",
      "q12-score    0.860   7.0\n",
      "q13-score    0.631   1.0\n",
      "q14-score    0.773   2.0\n",
      "total-score  0.856   6.0\n"
     ]
    }
   ],
   "source": [
    "df_scores.drop('id',axis=1, inplace = True)\n",
    "agg = df_scores.agg('sum')/(df_scores.agg('count')*10)\n",
    "agg['total-score'] = agg.values.mean()\n",
    "df_agg = agg.to_frame(name=\"mean\").round(3)\n",
    "df_agg['rank'] = agg.rank()\n",
    "\n",
    "\n",
    "print(df_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f5a42-7caa-4986-9d3b-71f4c4a23165",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Super-Simple Question Discrimination\n",
    "- How well the question helps identify high performers from low performers\n",
    "- LOTS of ways to do this, some sophisticated (to be reviewed later time permitting)\n",
    "- One of the simplest:  how does question correlate with overall test score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f55b25c3-cd7e-4908-84c3-5ea46a6bdcb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q02-score    0.472960\n",
      "q03-score    0.488587\n",
      "q04-score    0.481544\n",
      "q05-score    0.504897\n",
      "q06-score    0.610922\n",
      "q07-score    0.634638\n",
      "q08-score    0.618950\n",
      "q09-score    0.564130\n",
      "q10-score    0.570847\n",
      "q11-score    0.620656\n",
      "q12-score    0.614843\n",
      "q13-score    0.838059\n",
      "q14-score    0.728214\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "item_to_total_correlations = df_scores.corrwith(df_main['score'], method = 'spearman')\n",
    "print(item_to_total_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9b353-3357-44ef-9cab-1ed8b1ea2186",
   "metadata": {},
   "source": [
    "- Note use of Spearman's rho because score data are ordinal scale\n",
    "- Note further that use of full total score for correlation to individual question scores double counts each question's contribution\n",
    "- Thus, compute and adjusted total by subtracting each question from the overall total when computing that question's correlation, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40916ab-647a-4549-af79-445524977c56",
   "metadata": {},
   "source": [
    "### Adjusted question discrimination correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da68f4d7-ad23-4f08-b68a-4edbd6645d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q02-score    0.468658\n",
      "q03-score    0.486626\n",
      "q04-score    0.476920\n",
      "q05-score    0.491664\n",
      "q06-score    0.597306\n",
      "q07-score    0.619549\n",
      "q08-score    0.586859\n",
      "q09-score    0.558504\n",
      "q10-score    0.567403\n",
      "q11-score    0.592348\n",
      "q12-score    0.600645\n",
      "q13-score    0.558598\n",
      "q14-score    0.663751\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_adj_correl_totals = abs(df_scores.sub(df_main['score'], axis=0))\n",
    "adj_item_to_total_correlations = df_scores.corrwith(df_adj_correl_totals, method = 'spearman')\n",
    "print(adj_item_to_total_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3d3eb-5cd8-4d36-a1a4-44decfe45a6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bottom Line for Basic Item-Level Stats\n",
    "- Don't reveal all that much\n",
    "- Questions on the whole seem a bit on the easy order relative to taker capacity, which could be good or bad depending on nature of test\n",
    "- One potential counterexample to the 80/20 rule:  More sophisticated item-level analysis through Item Response Theory (IRT) may reveal more. See IRT notebook for details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
